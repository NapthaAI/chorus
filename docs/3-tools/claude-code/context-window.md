# Context Window Calculation

This document explains how context window usage is calculated from the Claude Agent SDK token metrics.

## Key Insight: Per-Message vs Cumulative Usage

The Claude API returns token usage in two places:

1. **Per-message usage** (`message.usage` on assistant/tool_use messages) - Shows the **current context window state** for that turn
2. **Result message usage** (`result.usage`) - Shows **cumulative billing tokens** across ALL turns

**IMPORTANT**: For context window calculation, use the **per-message usage**, NOT the result message usage. The result message aggregates all tokens billed across the session, which can exceed the context limit.

## Token Types

The Claude API returns several token types in the `usage` object:

| Token Type | Description | Counts Toward Context? |
|------------|-------------|------------------------|
| `input_tokens` | Tokens sent to Claude **after** the last cache breakpoint (uncached input) | Yes |
| `cache_read_input_tokens` | Tokens retrieved from the prompt cache (previously cached content) | Yes |
| `cache_creation_input_tokens` | Tokens being written to the cache for the first time | Yes |
| `output_tokens` | Tokens generated by Claude in its response | **No** |

## Context Window Formula

**The correct formula:**

```
context_used = input_tokens + cache_read_input_tokens + cache_creation_input_tokens
context_percentage = (context_used / context_limit) Ã— 100
```

**Output tokens are NOT included** because:
- The context window represents what is sent TO Claude (the input)
- `output_tokens` are what Claude generates in response
- Output tokens are billed separately but don't consume context window space

## Where to Get Usage Data

### For Context Window Calculation
Use the LAST assistant or tool_use message's usage:

```json
{
  "type": "assistant",
  "message": {
    "usage": {
      "input_tokens": 10,
      "cache_creation_input_tokens": 594,
      "cache_read_input_tokens": 110154,
      "output_tokens": 924
    }
  }
}
```

Context calculation: `10 + 594 + 110154 = 110,758 tokens` (~55% of 200k)

### For Session Stats (Cost, Turns, Duration)
Use the result message for billing/analytics data:

```json
{
  "type": "result",
  "total_cost_usd": 0.615,
  "num_turns": 21,
  "duration_ms": 79933,
  "usage": {
    "input_tokens": 9984,
    "cache_creation_input_tokens": 58679,
    "cache_read_input_tokens": 179083,
    "output_tokens": 3954
  }
}
```

**Warning**: The usage in result message is CUMULATIVE across all 21 turns. Do NOT use this for context window calculation - it would show 247k tokens which exceeds the 200k limit!

## Pricing vs Context

Important distinction:

| Aspect | `cache_read_input_tokens` Treatment |
|--------|-------------------------------------|
| **Pricing** | 90% cheaper than regular input tokens |
| **Context Window** | Counts 100% toward context limit |

Even though cached tokens are cheaper to process, they still occupy the same space in the context window as regular input tokens.

## Context Window Limits by Model

| Model | Context Window |
|-------|---------------|
| Claude Opus 4.5 | 200,000 tokens |
| Claude Sonnet 4.5 | 200,000 tokens (1M beta available) |
| Claude Haiku 4.5 | 200,000 tokens |

## Implementation

See `chorus/src/renderer/src/utils/context-limits.ts`:

```typescript
// Extract usage from the LAST assistant/tool_use message (current context state)
// NOT from result message (which is cumulative billing)
const claudeMsg = m.claudeMessage as { message?: { usage?: UsageData } }
const usage = claudeMsg?.message?.usage

// Context = input_tokens + cache_read_input_tokens + cache_creation_input_tokens
// Output tokens are NOT included
const contextUsed = inputTokens + cacheReadTokens + cacheCreationTokens
const contextPercentage = contextLimit > 0
  ? Math.min((contextUsed / contextLimit) * 100, 100)
  : 0
```

## Common Mistake

**Wrong**: Using result message's cumulative usage for context calculation
```
result.usage.input_tokens + result.usage.cache_read + result.usage.cache_creation
= 9984 + 179083 + 58679 = 247,746 tokens (WRONG - exceeds 200k!)
```

**Correct**: Using last assistant message's per-turn usage
```
assistant.message.usage.input_tokens + cache_read + cache_creation
= 10 + 110154 + 594 = 110,758 tokens (CORRECT - ~55% of 200k)
```

## References

- [How to Calculate Your Claude Code Context Usage](https://codelynx.dev/posts/calculate-claude-code-context) - Original reference (note: clarification needed on per-message vs cumulative)
- [Anthropic Prompt Caching Documentation](https://docs.anthropic.com/en/docs/build-with-claude/prompt-caching)
- [Anthropic Context Windows Documentation](https://docs.anthropic.com/en/docs/build-with-claude/context-windows)
