# Building a Collaborative Editor with LLM Agents: First-Principles Approach

## Understanding the Challenge: LLM Agents vs. Human Editors

Designing a Notion-like collaborative editor where **LLM agents** act as co-editors introduces unique challenges. Traditional real-time editors (e.g. Google Docs, Notion) assume humans typing characters or blocks incrementally. In contrast, LLM agents tend to produce **large, batch edits** – for example, rewriting an entire paragraph or section in one go rather than character-by-character keystrokes. This difference in editing granularity has significant implications for **concurrency control** and conflict resolution. We need to ensure all collaborators (human or AI) eventually see a consistent document state, be able to **trace changes** (who/what modified the text), resolve conflicts when simultaneous edits occur, and maintain high performance and responsiveness. Approaching this problem from first principles means examining the fundamental consistency models and data structures for collaborative editing, and then adapting them to the behavior of AI agents.

At its core, the problem is about managing a **shared state** (the document) under concurrent modifications by multiple independent “editors.” This is a well-studied distributed systems problem: we need a strategy to achieve *convergence* (everyone ends up with the same content) without overwriting each other's contributions. Key requirements include: **(1)** real-time responsiveness (low latency updates), **(2)** *deterministic convergence* (all replicas reach the same state) with **no lost updates**, **(3)** the ability to handle edits offline or out-of-order (eventual consistency), and **(4)** a mechanism to identify or resolve conflicting changes in a human-meaningful way. In addition, with AI in the loop, we should consider **monotonic progress** – ideally, one agent’s work shouldn’t be immediately undone by another agent’s concurrent action[\[1\]](https://arxiv.org/pdf/2510.18893#:~:text=substrate%20with%20three%20properties%3A%20,strong%20consistency%20via%20replicated%20logs%E2%80%94each). These requirements set the stage for evaluating different approaches like **CRDTs**, **Operational Transform (OT)**, **version-control style merges (Git)**, and hybrids.

## CRDTs for Real-Time, Lock-Free Merging

**Conflict-Free Replicated Data Types (CRDTs)** are a popular foundation for modern collaborative editors. A CRDT is a data structure that allows concurrent updates on multiple replicas and *guarantees eventual consistency* without needing strict coordination[\[2\]](https://dev.to/foxgem/crdts-achieving-eventual-consistency-in-distributed-systems-296g#:~:text=Summary). In practical terms, CRDTs enable *lock-free* collaboration: each editor can apply their operations locally and asynchronously propagate them; the CRDT’s merge logic ensures that, no matter the order of operations, all users converge to the same final document content[\[3\]\[3\]](https://shambhavishandilya.medium.com/understanding-real-time-collaboration-with-crdts-e764eb65024e#:~:text=CRDTs%20are%20a%20special%20kind,first%20mobile%20apps). This means changes don’t “clobber” each other – *every edit is incorporated in the final state in some way* (no operation is lost). Real-world systems like Figma’s multiplayer editor and some offline-capable note tools use CRDT-based sync for this reason[\[4\]](https://www.designgurus.io/blog/design-real-time-editor#:~:text=In%20summary%2C%20OT%20vs%20CRDT,as%20centralized%20vs%20decentralized%20approaches). In fact, Notion’s sync engine and many other collaborative apps employ CRDT or similar techniques for robust offline editing support[\[4\]](https://www.designgurus.io/blog/design-real-time-editor#:~:text=In%20summary%2C%20OT%20vs%20CRDT,as%20centralized%20vs%20decentralized%20approaches).

In a CRDT-based editor, the document is typically modeled as a sequence of elements (characters, or higher-level blocks). Each insertion or deletion is an operation tagged with a unique identifier so that operations can be applied commutatively. For example, the Yjs library (a high-performance CRDT implementation in JavaScript) represents text as a sequence CRDT (Y.Text) where each character or chunk has an ID; concurrent inserts at the same position get a deterministic order (often via a timestamp and replica ID ordering)[\[5\]](https://arxiv.org/pdf/2510.18893#:~:text=Table%202,4%20Agent%20Roles). If two edits happen at the “same” place, both will appear in the merged result in a consistent order, rather than one blindly overwriting the other. This **strong eventual consistency** property means no low-level merge conflicts ever need manual resolution – the algorithm resolves them automatically at the character or element level[\[6\]](https://arxiv.org/pdf/2510.18893#:~:text=,driven%20coordination). In a recent study, applying CRDTs to multi-agent LLM coding tasks guaranteed 100% convergence *with zero operation-level merge failures* (no manual merging at the character level)[\[7\]](https://arxiv.org/pdf/2510.18893#:~:text=%2882%E2%80%93189,0.71%29%20as%20assessed%20via).

**Implications for LLM Agents:** Using CRDTs would allow human users and LLM agents to edit concurrently with minimal locking. For instance, a human could be typing in one part of the doc while an AI agent rewrites another section; their operations will integrate without one blocking the other. Even if two agents do inadvertently edit the *exact same location* simultaneously, the CRDT will interleave their contributions in a consistent way (no crashes or dropped edits). In practice, a CRDT-based system could treat an AI agent’s large edit as a bundle of fine-grained operations (e.g. a series of character insertions/deletions constituting the rewrite). Libraries like **Yjs or Automerge** can handle thousands of operations per second and support data types for text, lists, maps, etc., making them suitable for document editing. In fact, one research prototype of a collaborative editor with human and AI co-authors used Yjs for real-time syncing[\[8\]](https://arxiv.org/html/2509.11826v1#:~:text=annotations.%20Real,fetching%20and%20local%20state%20management) – every change (whether from a user or an AI prompt result) was applied through the CRDT so that all participants’ views stayed in sync.

However, CRDTs are not a silver bullet. **Convergence** in CRDT means no conflicting low-level operations, but it doesn’t guarantee the merged text semantically makes sense. If two LLM agents rewrite the same sentence in different ways concurrently, the CRDT might merge them by intermixing bits of both edits (since from a character perspective, both are distinct inserts). The result could be garbled or at least not what either party intended. In technical terms, while CRDTs prevent *operation-level conflicts*, you can still end up with **semantic conflicts** – logically inconsistent or duplicate content that needs higher-level reconciliation[\[9\]](https://arxiv.org/pdf/2510.18893#:~:text=measurements%20cannot%20isolate,task%20characteristics%20including%20component%20interdependencies). In one experiment, even with CRDT ensuring zero lost updates, about 5–10% of documents had semantic issues that required cleanup (e.g. two agents duplicated the same section in different words)[\[9\]](https://arxiv.org/pdf/2510.18893#:~:text=measurements%20cannot%20isolate,task%20characteristics%20including%20component%20interdependencies). This implies that if we use CRDTs with AI agents, we might need an *additional layer* to detect and handle such cases (for example, flagging regions where simultaneous large edits occurred so a human or another AI can merge them coherently).

Another consideration is **performance and overhead**. Naively representing an AI’s full-document rewrite as a multitude of character operations could be inefficient. Fortunately, CRDT frameworks allow batching or compact representation of changes. For example, an AI agent’s edit could be applied as a block insert/delete operation (rather than character-by-character), which the CRDT can still merge by ordering that block against other operations. Modern CRDT algorithms (like Yjs, Fluid, Automerge v2) are quite optimized – they use *delta updates*, \**causal trees*, and other techniques to avoid ballooning network or memory usage even as documents grow[\[10\]](https://shambhavishandilya.medium.com/understanding-real-time-collaboration-with-crdts-e764eb65024e#:~:text=Operation,with%20Just%20the%20Edits)[\[11\]](https://shambhavishandilya.medium.com/understanding-real-time-collaboration-with-crdts-e764eb65024e#:~:text=Tradeoffs%20Explained). Still, it’s important to manage CRDT metadata: each unique insertion has an ID, so if an agent is rewriting large chunks frequently, the identifier list grows. Periodic compaction or snapshotting might be needed for a long-lived document to remain efficient.

**Traceability** is actually a strength of CRDTs: because every operation is applied in causal order and tagged with the author (replica ID), you effectively have a fine-grained history of all changes. This data can be leveraged to build a **“track changes” view** or audit log. You could aggregate the character-level ops into higher-level diffs per editing session – e.g. “Agent A replaced this sentence with that sentence at 3:45 PM.” Many CRDT systems don’t provide a user-facing history out of the box, but the information is there in the distributed log (some frameworks like Automerge allow querying history). In fact, researchers have noted that CRDTs store *more information than Git* about the timeline of edits (including the exact sequence and timing of every character insertion)[\[12\]](https://news.ycombinator.com/item?id=42343953#:~:text=Luckily%2C%20algorithms%20like%20egwalker%20have,users%20to%20manually%20resolve%20conflicts). This opens the door to detecting complex conflicts or providing rich undo/redo and history features. For example, one could use the causal history to detect when two branches of editing diverged on the same region and **mark that as a conflict range** despite the CRDT auto-merging them[\[12\]](https://news.ycombinator.com/item?id=42343953#:~:text=Luckily%2C%20algorithms%20like%20egwalker%20have,users%20to%20manually%20resolve%20conflicts). This idea essentially marries CRDT’s continuous merge with Git’s conflict markers for semantic clarity – though it’s an area of ongoing exploration.

## Operational Transform (OT) and Sequential Coordination

Before CRDTs rose in popularity, **Operational Transformation (OT)** was the classic approach (Google Docs historically uses OT). OT also allows real-time concurrent editing, but it relies on a central server (or a coordination layer) to **order and transform operations** in a consistent way for all clients. In an OT system, when operations race (e.g. two users insert at position 100 at the same time), the server will pick an order and adjust one operation’s position based on the other so that both can apply without conflict. This achieves the same goal of convergence, but through a different means: a centralized algorithm ensures each client applies operations in an agreed sequence (often by attaching a sequence number or version to each operation). In summary, **OT vs. CRDT** is often framed as **centralized vs. decentralized** solutions[\[4\]](https://www.designgurus.io/blog/design-real-time-editor#:~:text=In%20summary%2C%20OT%20vs%20CRDT,as%20centralized%20vs%20decentralized%20approaches). OT works well under a *single authority model* (a server as source of truth) and gives you strong consistency (everyone sees edits in the same order immediately)[\[13\]](https://www.designgurus.io/blog/design-real-time-editor#:~:text=Google%20Docs%20sticks%20with%20OT,more%20flexibility%20in%20offline%2Fdecentralized%20use)[\[14\]](https://www.designgurus.io/blog/design-real-time-editor#:~:text=Every%20edit%20operation%20from%20clients,goes%20to%20the%20server%20first). Its downside is that it can be complex to implement for all edge cases, and offline editing is harder (since a client must sync with the server before its changes are final). CRDTs, on the other hand, trade the strict immediate consistency for more **flexibility** – clients can merge changes peer-to-peer or offline, and no central point needs to totally order events[\[4\]](https://www.designgurus.io/blog/design-real-time-editor#:~:text=In%20summary%2C%20OT%20vs%20CRDT,as%20centralized%20vs%20decentralized%20approaches).

**Implications for our use case:** An OT-based or centralized approach could certainly be used with LLM agents as well. For example, one could design the system such that every edit (from a human or agent) goes through a server that sequentially applies them to a canonical document state before broadcasting to others[\[15\]](https://www.designgurus.io/blog/design-real-time-editor#:~:text=In%20our%20design%2C%20the%20server,authority%20on%20the%20document%E2%80%99s%20state)[\[16\]](https://www.designgurus.io/blog/design-real-time-editor#:~:text=The%20server%20applies%20operations%20in,when%20all%20operations%20are%20applied). In effect, this is a **“single timeline”** model: even if two edits happen close in time, one will be integrated slightly before the other and any overlapping changes will be resolved by transforming the later one’s indices or ranges. This approach guarantees a consistent state without divergent branches – there is no question of later merging because you never fork the state. It’s conceptually simpler to reason about final state (there’s one authoritative sequence of changes).

However, the **cost** is that truly concurrent editing is somewhat serialized. Agents (or users) might have to wait or re-fetch the latest state before applying their own changes if we enforce strict sequential consistency. For instance, if Agent A and Agent B both try to rewrite the same paragraph at once, one would “win” by arriving first to the server; the second agent’s change would be applied *on top of* the new text, potentially producing a weird outcome unless that agent was somehow aware of the first change and adjusted its output. In OT, that adjustment is exactly what happens: the second operation would be transformed against the first. If it was a simple character insertion, this is fine. If it was a complex rewrite, the transform might amount to doing a diff/merge anyway. In scenarios where LLM agents make large edits, the OT system effectively has to perform a merge-like operation in real-time (e.g. rebase one agent’s proposed text onto the other’s). This could be as complicated as what a CRDT would do – or it might just accept one and overwrite conflicting parts of the other (depending on the policy, possibly “last writer wins” for overlapping ranges, which could drop some content).

One advantage of a central coordinator is that you can implement **business rules or locks** more easily. For example, the system might decide to **lock a section** of the document while an AI agent is making an extensive edit, to prevent others from touching that section until it’s done (avoiding conflict altogether). This is akin to a coarse-grained manual concurrency control. It sacrifices some parallelism but simplifies the outcome. A central server could also orchestrate turns: e.g., let the human accept or reject an AI’s big change before allowing other edits – which might be desirable in knowledge work settings where a human-in-the-loop verifies AI contributions. In essence, an OT or sequential system leans toward *strong consistency* and potentially *higher precision in conflict handling*, at the expense of some flexibility and offline capability[\[4\]](https://www.designgurus.io/blog/design-real-time-editor#:~:text=In%20summary%2C%20OT%20vs%20CRDT,as%20centralized%20vs%20decentralized%20approaches). Performance-wise, OT algorithms are very fast for the scale of typical documents; the bottleneck is not computation but the requirement of a reliable low-latency network to feel seamless (usually via websockets for instant propagation)[\[17\]](https://www.designgurus.io/blog/design-real-time-editor#:~:text=,see%20their%20teammate%E2%80%99s%20latest%20sentence)[\[18\]](https://www.designgurus.io/blog/design-real-time-editor#:~:text=Using%20WebSockets%20,is%20key%20to%20low%20latency).

For our design, if we anticipate **frequent simultaneous edits by multiple agents**, a purely sequential model might become a bottleneck (agents waiting on each other). But if agent tasks are mostly turn-based or initiated by a human one at a time, a central OT engine could be perfectly sufficient and simpler. Notably, *Google Docs-style OT requires connectivity* – if we want the system to function during network interruptions or allow agents to work independently and sync later, CRDT or a version-control approach would be more suitable.

## Version Control (Git-Like) Model: Batches and Merges

Another conceptual model to consider is treating edits more like **Git commits** on branches that need merging. In this approach, instead of real-time operation-by-operation merging, each agent or human works on their own **version of the document**, and we merge the versions when needed. For example, imagine the document at state X. If an LLM agent is tasked with a substantial rewrite (say, “Summarize Section 2”), it could fetch the latest state X, apply its changes to produce a new document state X\_A, and submit that as a proposed update. Meanwhile another agent (or user) might have produced state X\_B with other edits. We then need to **merge X\_A and X\_B** relative to the base X, similar to how Git does a three-way merge on divergent branches. This approach treats large edits as atomic patches.

**Pros:** A big benefit here is clarity of changes and control. Each agent’s modifications can be encapsulated as a diff or patch, which is great for **traceability** – you can present each “commit” for review. Conflict resolution is also more *explicit*: if two patches touch the same lines in incompatible ways, you detect a conflict and can either automate a merge or ask for human intervention, much like merging two Git branches that both edited the same function. In fact, a deferred merge might be preferable for certain knowledge work tasks to maintain quality: instead of intermixing two AI outputs blindly, the system (or a human) can **choose one, combine them, or prompt an LLM to reconcile the differences**. This might yield a more semantically coherent result than automatic character-level weaving.

**Cons:** The drawback, of course, is that this is inherently more asynchronous. Collaborators won’t see each other’s changes until a merge happens. It’s less “real-time collaborative” and more like a turn-taking system unless merges are happening very frequently. If a human and an AI are supposed to be co-writing *simultaneously*, this model can introduce lag or the need for frequent small merges (at which point it starts to resemble CRDT/OT anyway). Another challenge is that performing merges can be expensive if done often – computing textual diffs and resolving conflicts for large documents is not trivial, especially if we try to do it automatically with another AI in the loop (which could introduce latency or errors). We also risk merge conflicts becoming frequent if multiple agents truly work in parallel on overlapping content. In a multi-agent coding context, a study found that **Git-style branching led to about 15–30% of tasks ending up with conflicts that had to be resolved at merge time**[\[19\]](https://arxiv.org/pdf/2510.18893#:~:text=execution%3B%20lock,while%20maintaining%20strong%20consistency%20guarantees%E2%80%94a). Each conflict resolution is effectively a manual or intelligent step needed, which could slow down the overall workflow or require additional logic (e.g. a “resolver” agent or human oversight).

One way to mitigate conflict frequency in a Git-like model is to **divide the work**: assign different sections or aspects of the document to different agents (so their “branches” rarely overlap). This is analogous to how developers try to avoid merge conflicts by working on different files or functions. In a document scenario, perhaps one agent focuses on grammar fixes throughout, while another expands content in a specific section – their changes might not collide. If collisions do occur, the system could detect it (e.g. two edits to the same sentence) and then invoke a resolution policy: pick one version as authoritative, or merge line-by-line with conflict markers for a human to clean up.

The **traceability** in this model is excellent: each merge or commit is a checkpoint that can be recorded. We can maintain a full version history (like a Git commit log) which is easily presented to the user. This satisfies the requirement to “trace changes” clearly – you can attribute each commit to an agent and show diffs. In fact, a CRDT system can simulate this by grouping operations into logical commits, but a Git model has it naturally.

**Hybrid Approaches:** We aren’t forced to choose pure CRDT/OT versus pure Git-style. There are hybrids where **real-time updates happen, but also with explicit checkpoints and conflict markers**. For example, one could use a CRDT for continuous sync *within* each branch, but if a user goes offline or an agent works separately for a while, treat that as a branch that must merge on return (here CRDT could even help auto-merge, or could mark conflicts). As Joseph Gentle (creator of ShareJS) suggests, since CRDT frameworks capture every edit in a DAG of operations, one could develop a merge algorithm that uses that extra info to flag conflict regions when two diverged branches are merged[\[12\]](https://news.ycombinator.com/item?id=42343953#:~:text=Luckily%2C%20algorithms%20like%20egwalker%20have,users%20to%20manually%20resolve%20conflicts). This would effectively bring **manual conflict resolution** into an “offline branch” scenario while still using CRDT underneath for fine-grained merging outside the conflict ranges. That kind of hybrid might give the best of both: real-time merging for non-conflicting changes and explicit resolution where truly incompatible edits were made.

From a first-principles perspective, the Git-like model aligns with an **optimistic concurrency** strategy: let changes happen independently and reconcile after the fact. It’s conceptually similar to how multi-version databases work or how manual editorial workflows run (editors work separately, then compare and merge documents). It might be appropriate if we expect larger, infrequent edits from agents that a human will review, rather than rapid-fire interwoven edits.

## Managing Conflicts and Coordination Strategies

No matter which core model we choose, we have to address **conflict resolution** at a level that makes sense for document content. Here are some strategies and their implications:

* **Avoidance via Coordination:** The simplest way to handle conflicts is to prevent them. This could mean **assigning distinct roles or sections** to agents. For example, if one LLM agent is tasked with Section A and another with Section B, there’s no textual overlap and thus no conflict. We can facilitate this by representing the document as structured blocks (like Notion’s blocks or Markdown sections) and using a **block-level lock or claim system**. In fact, the *CodeCRDT* system for multi-agent code generation did exactly this: it had an “Outliner” agent create placeholders (like TODOs), and “implementation” agents would claim a TODO before writing code into it[\[20\]](https://arxiv.org/pdf/2510.18893#:~:text=Figure%201,Deterministic%20ordering%20via%20operation%20ID)[\[21\]](https://arxiv.org/pdf/2510.18893#:~:text=Outliner%20generates%20TypeScript%2FReact%20skeletons%20with,status%3D%E2%80%99pending%E2%80%99). The claiming was done via a CRDT-managed map with **last-write-wins** semantics to ensure only one agent can claim a given task deterministically[\[22\]](https://arxiv.org/pdf/2510.18893#:~:text=Agents%20claim%20work%20via%20optimistic,agentId%2C%20claim%20succeeded%3B%20else%2C%20retry)[\[23\]](https://arxiv.org/pdf/2510.18893#:~:text=Safety%20Invariant%3A%20Yjs%20Y,converge%20to%20same%20winner%20deterministically). We could borrow that idea – an agent signals intention to edit a certain part of the doc (claiming it), and others then avoid that part until it’s done. This yields *monotonic progress*: no two agents will step on each other’s toes in the same paragraph at the same time, eliminating a class of conflicts altogether[\[1\]](https://arxiv.org/pdf/2510.18893#:~:text=substrate%20with%20three%20properties%3A%20,strong%20consistency%20via%20replicated%20logs%E2%80%94each).

* **Real-Time Observation and Adaptation:** Another novel approach (used in CodeCRDT) is to have agents *monitor the shared state* continuously and adapt their plans if they see someone else’s edit. In a collaborative setting, each agent (or even the human) could subscribe to document changes in real time[\[24\]](https://arxiv.org/pdf/2510.18893#:~:text=decades,1%29%20observable%20updates). If Agent B notices Agent A already edited a sentence that B was about to change, B can decide to skip or adjust its action, preventing duplication. This is an **observation-driven coordination** model[\[24\]](https://arxiv.org/pdf/2510.18893#:~:text=decades,1%29%20observable%20updates). It requires that the agents be capable of responding to partial updates. With LLMs, that might mean periodically re-reading the document state or being prompted with “content has changed, revise your output.” While today’s LLMs don’t natively support continuous input streaming, an orchestrator could chunk tasks so that after each chunk, the agent checks the latest doc. This pattern can reduce conflicting efforts but might introduce some overhead or latency (agents pausing to resynchronize).

* **Conflict Detection and Auto-Merge:** If two changes *do* conflict (e.g., both modified the same sentence differently), how do we detect and handle it? In a CRDT, the conflict would manifest as an odd-looking text (both versions present). We could have a post-process that recognizes when a segment of text was edited by two sources nearly simultaneously (using timestamps or the operation history) and mark that for review. Alternatively, if using version control merging, the conflict is detected during the merge (the diff/merge algorithm will flag overlapping edits). One could attempt an **automated merge** using another LLM: for example, feed the two versions of the sentence to a resolver model with instructions to produce a single, unified version. This is analogous to Git’s “merge tool” but powered by AI. It might work for smaller conflicts, but it’s not guaranteed to preserve all nuances correctly (and could introduce its own errors). Therefore, for mission-critical knowledge work, having a human review conflicts might be necessary – especially since the user indicated a human will be in the loop frequently.

* **Last-Write-Wins (LWW) vs. Intent Preservation:** A simpler fallback in some systems is *Last-Write-Wins*, where if two edits clash, the one with a later timestamp (or higher priority agent) simply overwrites the other. This keeps the system simple (no complex merge artifacts), but it sacrifices the principle of preserving everyone’s contributions (some work gets lost). Notion’s actual implementation for text collaboration reportedly relies on a form of last-writer-wins on the server in certain scenarios[\[25\]](https://brianlovin.com/hn/38289327#:~:text=You%20don%27t%20need%20a%20CRDT,wins%20decided%20by%20the%20server). Given our goals (trace changes, no lost edits), LWW is probably not ideal except maybe as a tiebreak for very low-level races (e.g., two agents insert a comma in the same place – one of them can win arbitrarily without much consequence).

## Document Model and Granularity Considerations

We should also think about the **document model**. Notion operates on a block model (each paragraph, list item, etc., is a block that can be synced/edited independently). For a Markdown document, we can similarly consider a hierarchical model (sections, paragraphs, sentences). Designing the system to operate on a *block or section granularity* can localize conflicts. For example, we might implement a CRDT at the block level (so the ordering of blocks is conflict-free) and a separate CRDT for content within each block. This way, two agents editing different blocks truly won’t interfere (different CRDT instances), and if two agents edit the same block, we know the conflict is confined to that block. Block-level operations (like moving or duplicating a section) can be handled as well – potentially with an array CRDT for the sequence of blocks.

If LLM agents often rewrite whole sections, treating a section as the basic unit can be useful. An agent’s rewrite might be modeled as “replace block X’s content with new content,” which can be broken down internally but conceptually we know it’s a block-level replacement. This could simplify presenting changes to the user (e.g., “AI rewrote this paragraph” rather than “AI inserted 57 characters and deleted 45 characters here”). High-level semantic diffing (sentences or paragraphs) will be more intelligible for knowledge work review.

**Performance** in terms of responsiveness should be fine with either approach if designed well. CRDTs have been shown to scale to large documents and many operations; techniques like *chunking and batching* keystrokes into one operation (e.g., send ops at 50ms intervals instead of per character) keep overhead low[\[26\]](https://www.designgurus.io/blog/design-real-time-editor#:~:text=Small%20optimizations%20make%20a%20big,difference%20at%20scale). In an AI scenario, since edits are naturally chunked (an AI outputs a paragraph all at once), we already have batching. The system should handle the insertion of, say, a 100-word paragraph as a single update broadcast to others, which is not a problem for modern algorithms. For perspective, a CRDT-based editor like Figma handles thousands of operations per second in complex documents.

A Git-like merge for large text might be slower if done repeatedly, but if agents are not literally editing every second, merges can be a background task. Additionally, merges can be scoped (only run diff on sections that changed). Using efficient diff algorithms or incremental diff (since we often know roughly where the change was) can mitigate performance issues.

Finally, we should plan for a **history mechanism**. In a system with heavy AI involvement, users will want to inspect or revert changes easily (e.g., “What did the AI agent change in my report?”). A robust version history (like Google Docs “Version history” or track-changes mode) will increase trust and usability. Implementing this could leverage whichever core model we choose: for CRDT/OT, we might still store periodic snapshots or commit-like markers (since those systems are usually aimed at real-time syncing, not long-term history). For a Git model, the commits *are* the history. In either case, storing a log of changes with timestamps and agent IDs is crucial. We can also present these changes in the UI in a human-friendly way (perhaps akin to Google Docs suggestions or Notion’s edit history).

## Conclusion and Conceptual Outlook

**From first principles**, the problem of multi-agent document editing boils down to distributed consensus on document state, tempered by human oversight and AI behavior patterns. The classical solutions (CRDTs, OT) give us a starting toolkit for consistency: CRDTs offer *eventual consistency* with maximum concurrency and no need for central arbitration, whereas OT/locking offers *immediate consistency* with a authoritative sequencer. A version-control approach treats each editor’s work as a separate branch to merge later, emphasizing clarity and explicit conflict handling.

In the context of LLM agents collaborating with humans, a likely **best approach** combines ideas from all these models: \- Use a **real-time sync engine** (CRDT-based or OT) so that straightforward, non-conflicting edits propagate instantly and no one’s work is lost. This ensures high responsiveness and a smooth collaborative feel. \- Impose some **structure or protocol** to prevent the worst conflicts – for example, partition the document into sections or tasks and have agents coordinate (through locks, “claim tickets”[\[22\]](https://arxiv.org/pdf/2510.18893#:~:text=Agents%20claim%20work%20via%20optimistic,agentId%2C%20claim%20succeeded%3B%20else%2C%20retry), or observing each other’s edits) to maintain *monotonic progress* (agents build on each other’s contributions rather than overwrite them)[\[1\]](https://arxiv.org/pdf/2510.18893#:~:text=substrate%20with%20three%20properties%3A%20,strong%20consistency%20via%20replicated%20logs%E2%80%94each). \- Incorporate a **conflict resolution layer** for semantic clashes. This might flag overlapping large edits and either defer to human judgment or use an AI merger. Even in a CRDT/OT system that merges automatically, surfacing a “conflict marker” for the user in ambiguous cases can be valuable (e.g., “Agent A and B edited this paragraph differently – please choose a version or merge”). This acknowledges that *conflict-free at the data level doesn’t always mean coherent outcome*. \- Maintain a **version history** and change trace. Each agent’s edits should be identifiable. This not only helps with transparency (the user can see what an AI changed) but also with debugging any issues (if an agent’s edit introduced an error, you can pinpoint it).

In implementation terms, one could start with a proven CRDT library (like Yjs or Automerge) to handle the heavy lifting of real-time merges and focus on building the higher-level features (history, conflict UI, agent coordination logic) on top. Indeed, an experiment integrating multiple AI writers in a shared editor found success using Yjs for syncing, with AI contributions added as comments or suggestions rather than direct edits to respect human control[\[27\]](https://arxiv.org/html/2509.11826v1#:~:text=Further%20work%20identified%20%E2%80%9Cwriting%20territory%E2%80%9D%C2%A0%28Larsen,comments%2C%20linking%20the%20two%20materials)[\[28\]](https://arxiv.org/html/2509.11826v1#:~:text=Building%20on%20this%2C%20our%20design,when%20saving%20the%20document). While that design chose to have AI outputs not automatically overwrite text (to avoid social friction), if we do allow direct AI edits, the underlying tech can handle it the same way it handles user keystrokes.

In summary, we should **think of each LLM agent as just another collaborator** in a distributed editing system. The fundamental constraints (consistency, conflict-free merges, low latency updates) remain the same as with human collaborators, but the **nature of edits** shifts the emphasis: larger, batched changes mean we must carefully manage how those changes integrate. CRDTs provide a solid foundation for “always merge, never lose data” which aligns well with not dropping any AI or human contribution[\[6\]](https://arxiv.org/pdf/2510.18893#:~:text=,driven%20coordination). Git-like versioning provides an intuitive way to present and audit those contributions. By combining these approaches – real-time merging for responsiveness and explicit conflict resolution for clarity – we can build a Notion-like editor that gracefully supports a mix of human and LLM agent edits. The solution will likely involve **multi-layered collaboration control**: low-level data sync, and high-level coordination protocols. Approaching it from first principles, we treat consistency and intention preservation as core, and then add mechanisms for performance (batching, partitioning the doc) and usability (traceable changes, user override on conflicts).

Ultimately, the goal is to let humans and AI **collaborate fluidly** on knowledge work documents, with the system ensuring that everyone’s contributions are integrated and visible, and that any collisions are handled transparently. By leveraging the decades of research in collaborative editing algorithms and the new capabilities of AI, we can design an editing environment where LLM agents become helpful co-authors rather than chaotic scribes.

**Sources:** Real-time collaborative editing algorithms and their trade-offs[\[4\]](https://www.designgurus.io/blog/design-real-time-editor#:~:text=In%20summary%2C%20OT%20vs%20CRDT,as%20centralized%20vs%20decentralized%20approaches)[\[19\]](https://arxiv.org/pdf/2510.18893#:~:text=execution%3B%20lock,while%20maintaining%20strong%20consistency%20guarantees%E2%80%94a); use of CRDTs (Yjs/Automerge) in multi-user and multi-agent editors[\[8\]](https://arxiv.org/html/2509.11826v1#:~:text=annotations.%20Real,fetching%20and%20local%20state%20management)[\[20\]](https://arxiv.org/pdf/2510.18893#:~:text=Figure%201,Deterministic%20ordering%20via%20operation%20ID); coordination techniques for multi-agent systems[\[24\]](https://arxiv.org/pdf/2510.18893#:~:text=decades,1%29%20observable%20updates)[\[22\]](https://arxiv.org/pdf/2510.18893#:~:text=Agents%20claim%20work%20via%20optimistic,agentId%2C%20claim%20succeeded%3B%20else%2C%20retry); handling merge conflicts and consistency in distributed edits[\[9\]](https://arxiv.org/pdf/2510.18893#:~:text=measurements%20cannot%20isolate,task%20characteristics%20including%20component%20interdependencies)[\[12\]](https://news.ycombinator.com/item?id=42343953#:~:text=Luckily%2C%20algorithms%20like%20egwalker%20have,users%20to%20manually%20resolve%20conflicts); system design considerations for collaborative editors[\[29\]](https://www.designgurus.io/blog/design-real-time-editor#:~:text=,be%20consistent%20for%20all%20collaborators)[\[15\]](https://www.designgurus.io/blog/design-real-time-editor#:~:text=In%20our%20design%2C%20the%20server,authority%20on%20the%20document%E2%80%99s%20state).

---

[\[1\]](https://arxiv.org/pdf/2510.18893#:~:text=substrate%20with%20three%20properties%3A%20,strong%20consistency%20via%20replicated%20logs%E2%80%94each) [\[5\]](https://arxiv.org/pdf/2510.18893#:~:text=Table%202,4%20Agent%20Roles) [\[6\]](https://arxiv.org/pdf/2510.18893#:~:text=,driven%20coordination) [\[7\]](https://arxiv.org/pdf/2510.18893#:~:text=%2882%E2%80%93189,0.71%29%20as%20assessed%20via) [\[9\]](https://arxiv.org/pdf/2510.18893#:~:text=measurements%20cannot%20isolate,task%20characteristics%20including%20component%20interdependencies) [\[19\]](https://arxiv.org/pdf/2510.18893#:~:text=execution%3B%20lock,while%20maintaining%20strong%20consistency%20guarantees%E2%80%94a) [\[20\]](https://arxiv.org/pdf/2510.18893#:~:text=Figure%201,Deterministic%20ordering%20via%20operation%20ID) [\[21\]](https://arxiv.org/pdf/2510.18893#:~:text=Outliner%20generates%20TypeScript%2FReact%20skeletons%20with,status%3D%E2%80%99pending%E2%80%99) [\[22\]](https://arxiv.org/pdf/2510.18893#:~:text=Agents%20claim%20work%20via%20optimistic,agentId%2C%20claim%20succeeded%3B%20else%2C%20retry) [\[23\]](https://arxiv.org/pdf/2510.18893#:~:text=Safety%20Invariant%3A%20Yjs%20Y,converge%20to%20same%20winner%20deterministically) [\[24\]](https://arxiv.org/pdf/2510.18893#:~:text=decades,1%29%20observable%20updates) CodeCRDT: Observation-Driven Coordination for Multi-Agent LLM Code Generation

[https://arxiv.org/pdf/2510.18893](https://arxiv.org/pdf/2510.18893)

[\[2\]](https://dev.to/foxgem/crdts-achieving-eventual-consistency-in-distributed-systems-296g#:~:text=Summary) CRDTs: Achieving Eventual Consistency in Distributed Systems \- DEV Community

[https://dev.to/foxgem/crdts-achieving-eventual-consistency-in-distributed-systems-296g](https://dev.to/foxgem/crdts-achieving-eventual-consistency-in-distributed-systems-296g)

[\[3\]](https://shambhavishandilya.medium.com/understanding-real-time-collaboration-with-crdts-e764eb65024e#:~:text=CRDTs%20are%20a%20special%20kind,first%20mobile%20apps) [\[10\]](https://shambhavishandilya.medium.com/understanding-real-time-collaboration-with-crdts-e764eb65024e#:~:text=Operation,with%20Just%20the%20Edits) [\[11\]](https://shambhavishandilya.medium.com/understanding-real-time-collaboration-with-crdts-e764eb65024e#:~:text=Tradeoffs%20Explained) Understanding real-time collaboration with CRDTs | by Shambhavi Shandilya | Medium

[https://shambhavishandilya.medium.com/understanding-real-time-collaboration-with-crdts-e764eb65024e](https://shambhavishandilya.medium.com/understanding-real-time-collaboration-with-crdts-e764eb65024e)

[\[4\]](https://www.designgurus.io/blog/design-real-time-editor#:~:text=In%20summary%2C%20OT%20vs%20CRDT,as%20centralized%20vs%20decentralized%20approaches) [\[13\]](https://www.designgurus.io/blog/design-real-time-editor#:~:text=Google%20Docs%20sticks%20with%20OT,more%20flexibility%20in%20offline%2Fdecentralized%20use) [\[14\]](https://www.designgurus.io/blog/design-real-time-editor#:~:text=Every%20edit%20operation%20from%20clients,goes%20to%20the%20server%20first) [\[15\]](https://www.designgurus.io/blog/design-real-time-editor#:~:text=In%20our%20design%2C%20the%20server,authority%20on%20the%20document%E2%80%99s%20state) [\[16\]](https://www.designgurus.io/blog/design-real-time-editor#:~:text=The%20server%20applies%20operations%20in,when%20all%20operations%20are%20applied) [\[17\]](https://www.designgurus.io/blog/design-real-time-editor#:~:text=,see%20their%20teammate%E2%80%99s%20latest%20sentence) [\[18\]](https://www.designgurus.io/blog/design-real-time-editor#:~:text=Using%20WebSockets%20,is%20key%20to%20low%20latency) [\[26\]](https://www.designgurus.io/blog/design-real-time-editor#:~:text=Small%20optimizations%20make%20a%20big,difference%20at%20scale) [\[29\]](https://www.designgurus.io/blog/design-real-time-editor#:~:text=,be%20consistent%20for%20all%20collaborators) How to Design a Real-Time Collaborative Document Editor

[https://www.designgurus.io/blog/design-real-time-editor](https://www.designgurus.io/blog/design-real-time-editor)

[\[8\]](https://arxiv.org/html/2509.11826v1#:~:text=annotations.%20Real,fetching%20and%20local%20state%20management) [\[27\]](https://arxiv.org/html/2509.11826v1#:~:text=Further%20work%20identified%20%E2%80%9Cwriting%20territory%E2%80%9D%C2%A0%28Larsen,comments%2C%20linking%20the%20two%20materials) [\[28\]](https://arxiv.org/html/2509.11826v1#:~:text=Building%20on%20this%2C%20our%20design,when%20saving%20the%20document) Collaborative Document Editing with Multiple Users and AI Agents

[https://arxiv.org/html/2509.11826v1](https://arxiv.org/html/2509.11826v1)

[\[12\]](https://news.ycombinator.com/item?id=42343953#:~:text=Luckily%2C%20algorithms%20like%20egwalker%20have,users%20to%20manually%20resolve%20conflicts) Lies I was told about collab editing, Part 1: Algorithms for offline editing | Hacker News

[https://news.ycombinator.com/item?id=42343953](https://news.ycombinator.com/item?id=42343953)

[\[25\]](https://brianlovin.com/hn/38289327#:~:text=You%20don%27t%20need%20a%20CRDT,wins%20decided%20by%20the%20server) You don't need a CRDT to build a collaborative experience

[https://brianlovin.com/hn/38289327](https://brianlovin.com/hn/38289327)