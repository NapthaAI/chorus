# Context Window Calculation

This document explains how context window usage is calculated from the Claude Agent SDK token metrics.

## Token Types

The Claude API returns several token types in the `usage` object of result messages:

| Token Type | Description |
|------------|-------------|
| `input_tokens` | Tokens sent to Claude **after** the last cache breakpoint (uncached input) |
| `cache_read_input_tokens` | Tokens retrieved from the prompt cache (previously cached content) |
| `cache_creation_input_tokens` | Tokens being written to the cache for the first time |
| `output_tokens` | Tokens generated by Claude in its response |

## Context Window Formula

**All token types count toward the context window limit:**

```
context_usage = (input_tokens + cache_read_input_tokens + cache_creation_input_tokens + output_tokens) / context_window × 100
```

Or equivalently:

```
total_input_tokens = input_tokens + cache_read_input_tokens + cache_creation_input_tokens
total_context_tokens = total_input_tokens + output_tokens
context_percentage = total_context_tokens / context_window × 100
```

## Important Distinctions

### Pricing vs Context

- **Pricing**: Cache reads cost only 10% of base input token price (significant cost savings)
- **Context Window**: Cache reads still consume 100% of the space in your context window

This is a common source of confusion. Even though `cache_read_input_tokens` are cheaper to process, they still occupy the same space in the context window as regular input tokens.

### Rate Limits vs Context Limits

- **Rate Limits (ITPM)**: Only `input_tokens + cache_creation_input_tokens` count toward rate limits
- **Context Window**: All token types count

## Context Window Limits by Model

| Model | Context Window |
|-------|---------------|
| Claude Opus 4.5 | 200,000 tokens |
| Claude Sonnet 4.5 | 200,000 tokens (1M beta available) |
| Claude Haiku 4.5 | 200,000 tokens |

The `contextWindow` value is available in the `modelUsage` field of result messages, so we use that rather than hardcoding limits.

## Example

Given this usage from a result message:

```json
{
  "usage": {
    "input_tokens": 12376,
    "cache_read_input_tokens": 153656,
    "cache_creation_input_tokens": 52670,
    "output_tokens": 4177
  },
  "modelUsage": {
    "claude-opus-4-5-20251101": {
      "contextWindow": 200000
    }
  }
}
```

Calculation:

```
total_input_tokens = 12,376 + 153,656 + 52,670 = 218,702
total_context_tokens = 218,702 + 4,177 = 222,879
context_percentage = 222,879 / 200,000 × 100 = 111.4%
```

In this example, the context is actually overflowing! Claude Code handles this by auto-compacting the conversation when approaching ~80% context usage.

## Implementation

See `chorus/src/renderer/src/utils/context-limits.ts` for the implementation:

```typescript
// Total input = all input token types (all count toward context)
const totalInputTokens = inputTokens + cacheReadTokens + cacheCreationTokens
// Total context = input + output (full context window usage)
const totalContextTokens = totalInputTokens + outputTokens
const contextPercentage = contextLimit > 0
  ? Math.min((totalContextTokens / contextLimit) * 100, 100)
  : 0
```

## References

- [Anthropic Prompt Caching Documentation](https://docs.claude.com/en/docs/build-with-claude/prompt-caching)
- [Anthropic Context Windows Documentation](https://docs.claude.com/en/docs/build-with-claude/context-windows)
- [Claude Code Context Calculation](https://codelynx.dev/posts/calculate-claude-code-context)
